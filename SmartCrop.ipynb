{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhumilshah26/precision_agriculture_ml/blob/main/SmartCrop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YiMhAmfIXNI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-kuzgKUMQ1H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCnyFskAJNnZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/SmartCropMergedDataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDdwuC_mJTrE"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9uN35ZkJ7ex"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(df.columns)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL7GsfNyJ_A_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check data types & missing values\n",
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jh0VYQoKfQi"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing essential values\n",
        "df = df.dropna(subset=['state_name', 'district_name', 'crop_year', 'season', 'crop', 'area_', 'production_'])\n",
        "\n",
        "# Remove zero or negative area or production\n",
        "df = df[(df['area_'] > 0) & (df['production_'] > 0)]\n",
        "\n",
        "# Create a yield column (tons per hectare, assuming same units)\n",
        "df['yield'] = df['production_'] / df['area_']\n",
        "\n",
        "# Standardize text\n",
        "for col in ['state_name', 'district_name', 'season', 'crop']:\n",
        "    df[col] = df[col].str.strip().str.title()\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO7W-S5oKliR"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkALAx1GKo7K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Top 10 crops by total area\n",
        "top_crops = df.groupby('crop')['area_'].sum().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_crops.values, y=top_crops.index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Crops by Total Area\")\n",
        "plt.xlabel(\"Total Area\")\n",
        "plt.ylabel(\"Crop\")\n",
        "plt.show()\n",
        "\n",
        "# Top 10 crops by production\n",
        "top_prod = df.groupby('crop')['production_'].sum().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_prod.values, y=top_prod.index, palette=\"mako\")\n",
        "plt.title(\"Top 10 Crops by Total Production\")\n",
        "plt.xlabel(\"Total Production\")\n",
        "plt.ylabel(\"Crop\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voGH1u-qKy5H"
      },
      "outputs": [],
      "source": [
        "# Year-wise total area & production\n",
        "yearly = df.groupby('crop_year')[['area_', 'production_']].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(data=yearly, x='crop_year', y='area_', label='Area', marker='o')\n",
        "sns.lineplot(data=yearly, x='crop_year', y='production_', label='Production', marker='s')\n",
        "plt.title(\"Yearly Trends of Total Area and Production\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AazkohgHK6XF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='season', order=df['season'].value_counts().index, palette='cubehelix')\n",
        "plt.title(\"Seasonal Distribution of Records\")\n",
        "plt.xlabel(\"Season\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Total production by season\n",
        "season_prod = df.groupby('season')['production_'].sum().sort_values(ascending=False)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=season_prod.index, y=season_prod.values, palette=\"coolwarm\")\n",
        "plt.title(\"Total Production by Season\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U6ndRYIK9fa"
      },
      "outputs": [],
      "source": [
        "# Top 10 states by total production\n",
        "state_prod = df.groupby('state_name')['production_'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=state_prod.values, y=state_prod.index, palette=\"crest\")\n",
        "plt.title(\"Top 10 States by Total Production\")\n",
        "plt.xlabel(\"Total Production\")\n",
        "plt.ylabel(\"State\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLuxdd-mLA3W"
      },
      "outputs": [],
      "source": [
        "# Average yield per crop\n",
        "avg_yield = df.groupby('crop')['yield'].mean().sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=avg_yield.values, y=avg_yield.index, palette=\"flare\")\n",
        "plt.title(\"Top 15 Crops by Average Yield\")\n",
        "plt.xlabel(\"Average Yield (Production/Area)\")\n",
        "plt.ylabel(\"Crop\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SLTFQExLFab"
      },
      "outputs": [],
      "source": [
        "corr = df[['area_', 'production_', 'yield']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='Blues')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf3QPP-8LMA4"
      },
      "outputs": [],
      "source": [
        "# Scatterplot to see if some yields are abnormally high\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(data=df, x='area_', y='yield', alpha=0.5)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(\"Yield vs Area (log scale)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfnbPlImMFkD"
      },
      "outputs": [],
      "source": [
        "pip install earthengine-api geemap geopandas geopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAzt9gSKPPab"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()  # follow the link and approve access\n",
        "\n",
        "# Replace 'your-project-id' below with your actual GEE project\n",
        "ee.Initialize(project='smartcrop-476713')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOBIOzQTRMN-"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Initialize(project='smartcrop-476713')\n",
        "\n",
        "# Test: print first Sentinel-2 image date globally\n",
        "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "first = s2.first()\n",
        "print(\"Sentinel-2 bands:\", first.bandNames().getInfo())\n",
        "print(\"First image date:\", first.date().format().getInfo())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9JIcccrRmB5"
      },
      "outputs": [],
      "source": [
        "chirps = ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\")\n",
        "print(\"CHIRPS sample:\", chirps.first().bandNames().getInfo())\n",
        "\n",
        "smap = ee.ImageCollection(\"NASA_USDA/HSL/SMAP10KM_soil_moisture\")\n",
        "print(\"SMAP sample:\", smap.first().bandNames().getInfo())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiowcC7gTJUp"
      },
      "outputs": [],
      "source": [
        "# PIPELINE: Place -> lat/lon -> fetch GEE timeseries -> indicators -> assessment\n",
        "# Run this in the same env where ee.Initialize(project='smartcrop-476713') already worked.\n",
        "\n",
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.geocoders import Nominatim\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "ee.Initialize(project='smartcrop-476713')  # ensure authenticated\n",
        "\n",
        "# -------------------------\n",
        "# 1) Geocode place name\n",
        "# -------------------------\n",
        "def geocode_place(place_name, user_agent='myapp'):\n",
        "    geolocator = Nominatim(user_agent=user_agent, timeout=10)\n",
        "    loc = geolocator.geocode(place_name, addressdetails=True)\n",
        "    if loc is None:\n",
        "        raise ValueError(f\"Place not found: {place_name}\")\n",
        "    return {'lat': loc.latitude, 'lon': loc.longitude, 'raw': loc.raw}\n",
        "\n",
        "# -------------------------\n",
        "# 2) Build buffer polygon (in meters)\n",
        "# -------------------------\n",
        "def buffer_point_geometry(lat, lon, radius_m=5000):\n",
        "    pt = ee.Geometry.Point(lon, lat)\n",
        "    poly = pt.buffer(radius_m)  # buffer in meters, Earth Engine uses meters\n",
        "    return poly\n",
        "\n",
        "# -------------------------\n",
        "# 3) Fetch time-series from GEE for date range\n",
        "#    Returns pandas DataFrame with daily (or aggregated) features\n",
        "# -------------------------\n",
        "def fetch_timeseries_gee(lat, lon, start_date, end_date, buffer_m=5000, daily=False):\n",
        "    geom = buffer_point_geometry(lat, lon, buffer_m)\n",
        "    start = ee.Date(start_date)\n",
        "    end   = ee.Date(end_date)\n",
        "\n",
        "    # Sentinel-2 NDVI (cloud-masked median per 5-day window to be robust)\n",
        "    def s2_mask_clouds(img):\n",
        "        # Keep it simple: use QA60 band if present; fallback no mask if not\n",
        "        qa = img.select('QA60')\n",
        "        mask = qa.lt(1)  # zero means no cloud bit flagged\n",
        "        return img.updateMask(mask)\n",
        "\n",
        "    s2col = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "            .filterDate(start, end)\n",
        "            .filterBounds(geom)\n",
        "            .map(lambda i: i.clip(geom))\n",
        "            .map(lambda i: i.normalizedDifference(['B8','B4']).rename('NDVI'))\n",
        "           )\n",
        "\n",
        "    # We'll aggregate NDVI into weekly medians to avoid cloud noise\n",
        "    def aggregate_weekly(col, start, end):\n",
        "        # build list of weekly intervals\n",
        "        start_dt = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "        end_dt   = datetime.strptime(end, \"%Y-%m-%d\")\n",
        "        weeks = []\n",
        "        d = start_dt\n",
        "        while d < end_dt:\n",
        "            wstart = ee.Date(d.strftime(\"%Y-%m-%d\"))\n",
        "            wend_dt = d + timedelta(days=7)\n",
        "            wend = ee.Date(min(wend_dt.strftime(\"%Y-%m-%d\"), end))\n",
        "            weeks.append((wstart, wend))\n",
        "            d = wend_dt\n",
        "        imgs = []\n",
        "        for (ws, we) in weeks:\n",
        "            sub = col.filterDate(ws, we)\n",
        "            # median NDVI for the window\n",
        "            med = sub.median().set('system:time_start', ws.millis())\n",
        "            imgs.append(med)\n",
        "        return ee.ImageCollection(imgs)\n",
        "\n",
        "    s2_weekly = aggregate_weekly(s2col, start_date, end_date)\n",
        "\n",
        "    # MODIS LST (Terra MOD11A1 daily LST at 1km)\n",
        "    modis = (ee.ImageCollection(\"MODIS/061/MOD11A1\")\n",
        "             .filterDate(start, end)\n",
        "             .filterBounds(geom)\n",
        "             .map(lambda i: i.select('LST_Day_1km').multiply(0.02).rename('LST'))  # scale 0.02 -> K, convert to C later\n",
        "            )\n",
        "\n",
        "    # CHIRPS precipitation daily (mm)\n",
        "    chirps = (ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\")\n",
        "              .filterDate(start, end)\n",
        "              .filterBounds(geom)\n",
        "              .select('precipitation')\n",
        "             )\n",
        "\n",
        "    # SMAP soil moisture product (NASA_USDA/HSL/SMAP10KM_soil_moisture) uses 'ssm'\n",
        "    smap = (ee.ImageCollection(\"NASA_USDA/HSL/SMAP10KM_soil_moisture\")\n",
        "            .filterDate(start, end)\n",
        "            .filterBounds(geom)\n",
        "            .select('ssm')\n",
        "           )\n",
        "\n",
        "    # ERA5 daily 2m temperature and surface solar radiation downwards (ssrd)\n",
        "    era5 = (ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
        "           .filterDate(start, end)\n",
        "           .filterBounds(geom)\n",
        "           )\n",
        "    # era5 bands include 'mean_2m_air_temperature' (Kelvin), 'surface_solar_radiation_downwards' (J m-2)\n",
        "    # if not present, use ERA5-Land collection name variations as available\n",
        "\n",
        "    # helper to reduce collection into a pandas series keyed by date (system:time_start)\n",
        "    def collection_to_timeseries(col, band_name, scale):\n",
        "        def image_to_dict(img):\n",
        "            date = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd')\n",
        "            stat = img.reduceRegion(ee.Reducer.mean(), geom, scale=scale, bestEffort=True).get(band_name)\n",
        "            return ee.Feature(None, {'date': date, band_name: stat})\n",
        "        feats = col.map(image_to_dict).filter(ee.Filter.notNull([band_name]))\n",
        "        # export as list\n",
        "        features_list = feats.getInfo()['features']\n",
        "        rows = []\n",
        "        for f in features_list:\n",
        "            d = f['properties']['date']\n",
        "            v = f['properties'][band_name]\n",
        "            rows.append((d, v))\n",
        "        df = pd.DataFrame(rows, columns=['date', band_name])\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df[band_name] = pd.to_numeric(df[band_name], errors='coerce')\n",
        "        df = df.sort_values('date').reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    # Convert weekly NDVI collection to timeseries\n",
        "    ndvi_df = collection_to_timeseries(s2_weekly, 'NDVI', scale=1000)  # scale 1km for speedy reduce\n",
        "    lst_df  = collection_to_timeseries(modis, 'LST', scale=1000)\n",
        "    precip_df = collection_to_timeseries(chirps, 'precipitation', scale=5566)\n",
        "    smap_df   = collection_to_timeseries(smap, 'ssm', scale=10000)\n",
        "    # ERA5: try to fetch mean_2m_air_temperature and surface_solar_radiation_downwards\n",
        "    try:\n",
        "        t2m_df = collection_to_timeseries(era5.select('mean_2m_air_temperature'), 'mean_2m_air_temperature', scale=10000)\n",
        "        # Kelvin -> Celsius\n",
        "        t2m_df['mean_2m_air_temperature'] = t2m_df['mean_2m_air_temperature'] - 273.15\n",
        "    except Exception as e:\n",
        "        print(\"ERA5 2m temp failed:\", e)\n",
        "        t2m_df = pd.DataFrame(columns=['date','mean_2m_air_temperature'])\n",
        "\n",
        "    try:\n",
        "        ssrd_df = collection_to_timeseries(era5.select('surface_solar_radiation_downwards'), 'surface_solar_radiation_downwards', scale=10000)\n",
        "        # convert J/m2 per day to kWh/m2/day: 1 J = 2.77778e-7 kWh\n",
        "        ssrd_df['ssrd_kwh_m2'] = ssrd_df['surface_solar_radiation_downwards'] * 2.77778e-7\n",
        "    except Exception as e:\n",
        "        print(\"ERA5 ssrd failed:\", e)\n",
        "        ssrd_df = pd.DataFrame(columns=['date','surface_solar_radiation_downwards','ssrd_kwh_m2'])\n",
        "\n",
        "    # Merge timeseries on date using outer join (then forward/backfill small gaps)\n",
        "    df = ndvi_df.merge(lst_df, on='date', how='outer') \\\n",
        "                .merge(precip_df, on='date', how='outer') \\\n",
        "                .merge(smap_df, on='date', how='outer') \\\n",
        "                .merge(t2m_df, on='date', how='outer') \\\n",
        "                .merge(ssrd_df[['date','ssrd_kwh_m2']], on='date', how='outer')\n",
        "\n",
        "    # fill small gaps (interpolate)\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    df.interpolate(method='time', inplace=True, limit=3)\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "\n",
        "    # convert LST (MODIS) from scaled Kelvin to Celsius if present\n",
        "    if 'LST' in df.columns:\n",
        "        # MODIS LST_Day_1km scaled by 0.02 -> Kelvin\n",
        "        df['LST_C'] = df['LST'] * 0.02 - 273.15\n",
        "\n",
        "    # ensure numeric types\n",
        "    for c in df.columns:\n",
        "        if c != 'date':\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "    # optionally return daily vs weekly (we used weekly NDVI; you can resample)\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# 4) Indicators & heuristic assessment\n",
        "# -------------------------\n",
        "def compute_indicators(df):\n",
        "    # assume df has 'date' and at least ndvi, ssm, precipitation, mean_2m_air_temperature, LST_C, ssrd_kwh_m2\n",
        "    out = {}\n",
        "    # rolling seasonal stats (choose 30-day window)\n",
        "    df = df.set_index('date').sort_index()\n",
        "\n",
        "    window = 30\n",
        "    # median/mean levels\n",
        "    out['ndvi_mean'] = df['NDVI'].mean() if 'NDVI' in df else np.nan\n",
        "    out['ndvi_trend_slope'] = np.nan\n",
        "    try:\n",
        "        # trend slope (linear fit of NDVI over time)\n",
        "        x = np.arange(len(df))\n",
        "        y = df['NDVI'].values\n",
        "        mask = ~np.isnan(y)\n",
        "        if mask.sum() > 2:\n",
        "            slope = np.polyfit(x[mask], y[mask], 1)[0]\n",
        "            out['ndvi_trend_slope'] = slope\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    out['smap_mean'] = df['ssm'].mean() if 'ssm' in df else np.nan\n",
        "    out['smap_trend'] = np.nan\n",
        "    try:\n",
        "        y = df['ssm'].values\n",
        "        mask = ~np.isnan(y)\n",
        "        if mask.sum() > 2:\n",
        "            out['smap_trend'] = np.polyfit(x[mask], y[mask], 1)[0]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    out['precip_total'] = df['precipitation'].sum() if 'precipitation' in df else np.nan\n",
        "    out['precip_mean'] = df['precipitation'].mean() if 'precipitation' in df else np.nan\n",
        "    out['t2m_mean'] = df['mean_2m_air_temperature'].mean() if 'mean_2m_air_temperature' in df else np.nan\n",
        "    out['lst_mean'] = df['LST_C'].mean() if 'LST_C' in df else np.nan\n",
        "    out['ssrd_mean_kwh_m2'] = df['ssrd_kwh_m2'].mean() if 'ssrd_kwh_m2' in df else np.nan\n",
        "\n",
        "    # simple anomalies relative to rolling historical mean (if df contains multi-year, you can compute long-term)\n",
        "    out['ndvi_anom'] = df['NDVI'].iloc[-1] - df['NDVI'].mean() if 'NDVI' in df else np.nan\n",
        "\n",
        "    return out\n",
        "\n",
        "def assess_crop_health(indicators):\n",
        "    \"\"\"\n",
        "    Heuristic scoring. Positive score => improving, negative => depreciating.\n",
        "    Weighted contributions:\n",
        "      - NDVI trend (most important)\n",
        "      - Soil moisture trend\n",
        "      - Precipitation anomaly\n",
        "      - Temperature stress (very high LST or T2M reduces score)\n",
        "      - Solar radiation (gives energy)\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "    # weights (hand-tuned â€” you can adjust)\n",
        "    w_ndvi = 5.0\n",
        "    w_smap = 3.0\n",
        "    w_precip = 1.0\n",
        "    w_temp = -0.8\n",
        "    w_ssrd = 0.5\n",
        "\n",
        "    ndvi_s = indicators.get('ndvi_trend_slope', 0) or 0\n",
        "    smap_s = indicators.get('smap_trend', 0) or 0\n",
        "    precip_total = indicators.get('precip_total', 0) or 0\n",
        "    t2m = indicators.get('t2m_mean', np.nan)\n",
        "    ssrd = indicators.get('ssrd_mean_kwh_m2', 0) or 0\n",
        "\n",
        "    # Normalize signals roughly (ad-hoc):\n",
        "    score += w_ndvi * (ndvi_s * 100)   # NDVI slope is small, scale it\n",
        "    score += w_smap * (smap_s * 100)   # smap slope is small\n",
        "    # precipitation: relative to 30-day mean, but we only have current; use total\n",
        "    score += w_precip * math.log1p(precip_total)\n",
        "    if not math.isnan(t2m):\n",
        "        # penalize if mean temp > 35C OR <5C (crop dependent; this is generic)\n",
        "        if t2m > 35:\n",
        "            score += w_temp * (t2m - 35)\n",
        "        elif t2m < 5:\n",
        "            score += w_temp * (5 - t2m)\n",
        "    score += w_ssrd * ssrd\n",
        "\n",
        "    # map to label\n",
        "    if score > 3:\n",
        "        label = 'Improving'\n",
        "    elif score < -3:\n",
        "        label = 'Depreciating'\n",
        "    else:\n",
        "        label = 'Stable'\n",
        "\n",
        "    return {'score': score, 'label': label}\n",
        "\n",
        "# -------------------------\n",
        "# 5) Full wrapper: place -> assessment\n",
        "# -------------------------\n",
        "def assess_place(place_name, start_date, end_date, buffer_m=5000):\n",
        "    g = geocode_place(place_name)\n",
        "    lat, lon = g['lat'], g['lon']\n",
        "    print(f\"Place: {place_name} -> lat {lat:.4f}, lon {lon:.4f}\")\n",
        "    ts = fetch_timeseries_gee(lat, lon, start_date, end_date, buffer_m=buffer_m)\n",
        "    inds = compute_indicators(ts)\n",
        "    result = assess_crop_health(inds)\n",
        "    # attach some series for plotting\n",
        "    result['timeseries'] = ts\n",
        "    result['indicators'] = inds\n",
        "    return result\n",
        "\n",
        "# -------------------------\n",
        "# Example usage\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    place = \"Ahmedabad, India\"\n",
        "    # example: last 1 year\n",
        "    end = datetime.utcnow().date()\n",
        "    start = end - timedelta(days=365)\n",
        "    res = assess_place(place, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\"), buffer_m=10000)\n",
        "    print(\"Assessment label:\", res['label'])\n",
        "    print(\"Score:\", res['score'])\n",
        "    print(\"Key indicators:\", res['indicators'])\n",
        "\n",
        "    # Quick plot NDVI and soil moisture over time\n",
        "    ts = res['timeseries']\n",
        "    fig, ax = plt.subplots(2,1, figsize=(10,6), sharex=True)\n",
        "    if 'NDVI' in ts:\n",
        "        ax[0].plot(ts['date'], ts['NDVI'], marker='o', label='NDVI')\n",
        "        ax[0].legend()\n",
        "    if 'ssm' in ts:\n",
        "        ax[1].plot(ts['date'], ts['ssm'], marker='o', label='Soil moisture (SMAP)')\n",
        "        ax[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# End of pipeline\n",
        "# -------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTSdmGjmTyE1",
        "outputId": "cd7e62f7-ae74-4f53-81ee-dd137155a076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=10)\")': /search?q=Tirap%2C+Arunachal+Pradesh%2C+India&format=json&limit=1\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=10)\")': /search?q=Karbi+Anglong%2C+Assam%2C+India&format=json&limit=1\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=10)\")': /search?q=Lakhimpur%2C+Assam%2C+India&format=json&limit=1\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import ee\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "ee.Initialize(project='smartcrop-476713')\n",
        "\n",
        "# --- Helper 1: Get lat/lon from district/state ---\n",
        "geolocator = Nominatim(user_agent=\"smartcrop\", timeout=10)\n",
        "\n",
        "def get_latlon(state, district):\n",
        "    query = f\"{district}, {state}, India\"\n",
        "    try:\n",
        "        loc = geolocator.geocode(query)\n",
        "        if loc:\n",
        "            return loc.latitude, loc.longitude\n",
        "    except Exception:\n",
        "        pass\n",
        "    return np.nan, np.nan\n",
        "\n",
        "\n",
        "# --- Helper 2: Fetch remote sensing seasonal features from GEE ---\n",
        "def get_remote_features(lat, lon, crop_year, season):\n",
        "    \"\"\"Fetch seasonal averages from GEE for one point.\"\"\"\n",
        "    try:\n",
        "        # convert to EE geometry\n",
        "        geom = ee.Geometry.Point(lon, lat)\n",
        "\n",
        "        # Define season windows (adjust if needed)\n",
        "        if season.lower() == 'kharif':\n",
        "            start = ee.Date.fromYMD(int(crop_year), 6, 1)\n",
        "            end   = ee.Date.fromYMD(int(crop_year), 10, 31)\n",
        "        elif season.lower() == 'rabi':\n",
        "            start = ee.Date.fromYMD(int(crop_year), 10, 1)\n",
        "            end   = ee.Date.fromYMD(int(crop_year)+1, 3, 31)\n",
        "        else:  # whole year\n",
        "            start = ee.Date.fromYMD(int(crop_year), 1, 1)\n",
        "            end   = ee.Date.fromYMD(int(crop_year), 12, 31)\n",
        "\n",
        "        # Sentinel-2 NDVI\n",
        "        s2 = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "              .filterDate(start, end)\n",
        "              .filterBounds(geom)\n",
        "              .map(lambda img: img.normalizedDifference(['B8', 'B4']).rename('NDVI')))\n",
        "        ndvi_mean = s2.select('NDVI').mean().reduceRegion(\n",
        "            reducer=ee.Reducer.mean(), geometry=geom, scale=1000, bestEffort=True).get('NDVI')\n",
        "\n",
        "        # SMAP soil moisture\n",
        "        smap = (ee.ImageCollection(\"NASA_USDA/HSL/SMAP10KM_soil_moisture\")\n",
        "                .filterDate(start, end)\n",
        "                .filterBounds(geom)\n",
        "                .select('ssm'))\n",
        "        smap_mean = smap.mean().reduceRegion(\n",
        "            ee.Reducer.mean(), geom, scale=10000, bestEffort=True).get('ssm')\n",
        "\n",
        "        # CHIRPS precipitation\n",
        "        chirps = (ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\")\n",
        "                  .filterDate(start, end)\n",
        "                  .filterBounds(geom)\n",
        "                  .select('precipitation'))\n",
        "        precip_sum = chirps.sum().reduceRegion(\n",
        "            ee.Reducer.mean(), geom, scale=5566, bestEffort=True).get('precipitation')\n",
        "\n",
        "        # ERA5 temperature and solar radiation\n",
        "        era5 = (ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\n",
        "                .filterDate(start, end)\n",
        "                .filterBounds(geom))\n",
        "        temp_mean = era5.select('mean_2m_air_temperature').mean().reduceRegion(\n",
        "            ee.Reducer.mean(), geom, scale=10000, bestEffort=True).get('mean_2m_air_temperature')\n",
        "        solar_mean = era5.select('surface_solar_radiation_downwards').mean().reduceRegion(\n",
        "            ee.Reducer.mean(), geom, scale=10000, bestEffort=True).get('surface_solar_radiation_downwards')\n",
        "\n",
        "        # Convert results to Python types\n",
        "        result = {\n",
        "            'ndvi_mean': ee.Number(ndvi_mean).getInfo() if ndvi_mean else np.nan,\n",
        "            'soil_moisture': ee.Number(smap_mean).getInfo() if smap_mean else np.nan,\n",
        "            'precip_total': ee.Number(precip_sum).getInfo() if precip_sum else np.nan,\n",
        "            'temp_mean_c': (ee.Number(temp_mean).getInfo() - 273.15) if temp_mean else np.nan,\n",
        "            'sunshine_kwh_m2': ee.Number(solar_mean).getInfo() * 2.77778e-7 if solar_mean else np.nan\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error for ({lat},{lon},{crop_year},{season}):\", e)\n",
        "        return {'ndvi_mean': np.nan, 'soil_moisture': np.nan,\n",
        "                'precip_total': np.nan, 'temp_mean_c': np.nan,\n",
        "                'sunshine_kwh_m2': np.nan}\n",
        "\n",
        "\n",
        "# --- Step 1: Prepare unique keys (district-season-year) ---\n",
        "unique_keys = df[['state_name','district_name','crop_year','season']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Add lat/lon for each district (only once per state/district)\n",
        "coords = []\n",
        "for _, row in unique_keys.iterrows():\n",
        "    lat, lon = get_latlon(row['state_name'], row['district_name'])\n",
        "    coords.append((lat, lon))\n",
        "    time.sleep(1)  # be nice to Nominatim\n",
        "unique_keys['lat'], unique_keys['lon'] = zip(*coords)\n",
        "\n",
        "# Drop those with missing coordinates\n",
        "unique_keys = unique_keys.dropna(subset=['lat','lon']).reset_index(drop=True)\n",
        "\n",
        "# --- Step 2: Fetch environmental features for each unique row ---\n",
        "env_data = []\n",
        "for i, row in unique_keys.head(5).iterrows():   # or .sample(5)\n",
        "    print(f\"[{i+1}/{len(unique_keys)}] {row['district_name']}, {row['crop_year']} {row['season']}\")\n",
        "    feats = get_remote_features(row['lat'], row['lon'], row['crop_year'], row['season'])\n",
        "    env_data.append(feats)\n",
        "\n",
        "env_df = pd.DataFrame(env_data)\n",
        "merged_env = pd.concat([unique_keys, env_df], axis=1)\n",
        "\n",
        "# --- Step 3: Merge back with main df ---\n",
        "df_merged = df.merge(merged_env, on=['state_name','district_name','crop_year','season'], how='left')\n",
        "\n",
        "# --- Step 4: Save for later ML analysis ---\n",
        "df_merged.to_csv(\"cropdata_with_env_features.csv\", index=False)\n",
        "print(\"âœ… Final dataset saved as cropdata_with_env_features.csv\")\n",
        "\n",
        "print(df_merged.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6689Iae8WY8b"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Load merged dataset ---\n",
        "df = pd.read_csv(\"cropdata_with_env_features.csv\")\n",
        "\n",
        "# --- Basic cleaning ---\n",
        "# Drop missing or invalid values\n",
        "df = df.dropna(subset=['yield','ndvi_mean','soil_moisture','precip_total','temp_mean_c'])\n",
        "df = df.replace([np.inf,-np.inf], np.nan).dropna()\n",
        "\n",
        "# Quick look\n",
        "print(\"Rows:\", len(df))\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfUsVVdZWbdJ"
      },
      "outputs": [],
      "source": [
        "# --- Encode categorical columns ---\n",
        "df['season'] = df['season'].astype('category').cat.codes\n",
        "df['crop'] = df['crop'].astype('category').cat.codes\n",
        "df['state_name'] = df['state_name'].astype('category').cat.codes\n",
        "df['district_name'] = df['district_name'].astype('category').cat.codes\n",
        "\n",
        "# --- Feature selection ---\n",
        "features = [\n",
        "    'state_name', 'district_name', 'crop_year', 'season', 'crop',\n",
        "    'area_', 'ndvi_mean', 'soil_moisture', 'precip_total',\n",
        "    'temp_mean_c', 'sunshine_kwh_m2'\n",
        "]\n",
        "target = 'yield'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dMRuNPvWfwe"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=15,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYcjuXiKWk2T"
      },
      "outputs": [],
      "source": [
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"RÂ²: {r2:.3f}\")\n",
        "print(f\"MAE: {mae:.3f}\")\n",
        "print(f\"RMSE: {rmse:.3f}\")\n",
        "\n",
        "# Plot predicted vs actual yield\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.xlabel(\"Actual Yield\")\n",
        "plt.ylabel(\"Predicted Yield\")\n",
        "plt.title(\"Predicted vs Actual Yield\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELhZOewfWnKz"
      },
      "outputs": [],
      "source": [
        "# --- Simple importance ---\n",
        "importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=importances.values, y=importances.index, palette=\"viridis\")\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rZMRYoJWpYS"
      },
      "outputs": [],
      "source": [
        "# Initialize SHAP explainer\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# --- Summary plot ---\n",
        "shap.summary_plot(shap_values, X_test, feature_names=features)\n",
        "\n",
        "# --- Dependence plot (example for NDVI) ---\n",
        "shap.dependence_plot(\"ndvi_mean\", shap_values, X_test, feature_names=features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bga4kT9SWsBr"
      },
      "outputs": [],
      "source": [
        "# Suppose you have current-season environmental features for a district\n",
        "new_sample = {\n",
        "    'state_name': 10,  # encoded value or same as training mapping\n",
        "    'district_name': 55,\n",
        "    'crop_year': 2025,\n",
        "    'season': 1,       # e.g. 0 = Kharif, 1 = Rabi\n",
        "    'crop': 4,         # e.g. Rice\n",
        "    'area_': 1500,\n",
        "    'ndvi_mean': 0.52,\n",
        "    'soil_moisture': 0.18,\n",
        "    'precip_total': 740,\n",
        "    'temp_mean_c': 30.5,\n",
        "    'sunshine_kwh_m2': 5.1\n",
        "}\n",
        "\n",
        "sample_df = pd.DataFrame([new_sample])\n",
        "pred_yield = rf.predict(sample_df)[0]\n",
        "print(f\"ðŸŒ¾ Predicted yield: {pred_yield:.2f} (units same as training data)\")\n",
        "\n",
        "\n",
        "rf = joblib.load(\"crop_yield_model.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}